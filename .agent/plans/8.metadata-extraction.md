# Plan: Module 4 - Metadata Extraction

**Complexity:** ⚠️ Medium

## Summary

Implement LLM-powered metadata extraction during document ingestion. When a document is uploaded, an LLM analyzes the content and extracts structured metadata (document type, topics, entities, date references, etc.). Store this metadata alongside documents and chunks, then enable metadata-filtered retrieval. This allows queries like "Find Python tutorials from 2023" or "Show me financial reports about Q3 earnings."

**Learning Goals:**
- Structured extraction with Pydantic schemas
- Metadata design for retrieval enhancement
- Metadata filtering in vector search
- When to extract metadata vs when it's overkill

---

## Current Problem

The existing RAG system retrieves documents purely by semantic similarity:
1. User asks: "Show me Python tutorials from last year"
2. System embeds query, does vector search
3. Returns semantically similar chunks - but no way to filter by:
   - Document type (tutorial vs reference vs blog post)
   - Publication date
   - Programming language mentioned
   - Author or source
   - Topic category

**Impact:**
- Can't filter by attributes users care about
- Returns relevant content from wrong time periods or categories
- No way to narrow search beyond semantic similarity
- Poor precision on specific queries

---

## Solution Design

### 1. Metadata Schema

Define a Pydantic schema for extracted metadata:
```python
from pydantic import BaseModel, Field
from typing import Literal

class DocumentMetadata(BaseModel):
    """Structured metadata extracted from document content."""

    document_type: Literal[
        "tutorial", "guide", "reference", "blog_post",
        "research_paper", "documentation", "report", "other"
    ] = Field(description="Type of document")

    topics: list[str] = Field(
        description="Main topics/subjects covered (max 5)",
        max_length=5
    )

    programming_languages: list[str] = Field(
        default_factory=list,
        description="Programming languages mentioned (if any)"
    )

    frameworks_tools: list[str] = Field(
        default_factory=list,
        description="Frameworks, libraries, or tools mentioned (max 5)",
        max_length=5
    )

    date_references: str | None = Field(
        default=None,
        description="Any date or time period mentioned in content (e.g., '2023', 'Q3 2024', 'January 2025')"
    )

    key_entities: list[str] = Field(
        default_factory=list,
        description="Important people, companies, or organizations mentioned (max 5)",
        max_length=5
    )

    summary: str = Field(
        description="One-sentence summary of document content",
        max_length=200
    )

    technical_level: Literal["beginner", "intermediate", "advanced", "expert"] = Field(
        description="Technical difficulty level"
    )
```

### 2. Extraction Service

Create a service that uses structured outputs to extract metadata:
```python
# backend/app/services/metadata_service.py

from openai import AsyncOpenAI
from app.models.schemas import DocumentMetadata

async def extract_metadata(text_content: str, filename: str) -> DocumentMetadata:
    """
    Extract structured metadata from document content using LLM.

    Args:
        text_content: Full document text
        filename: Original filename (may contain hints)

    Returns:
        DocumentMetadata object with extracted fields
    """
    client = AsyncOpenAI(
        base_url=settings.llm_base_url,
        api_key=settings.llm_api_key
    )

    # Use structured outputs (response_format)
    completion = await client.beta.chat.completions.parse(
        model=settings.llm_model,
        messages=[{
            "role": "user",
            "content": f"""Analyze this document and extract structured metadata.

Filename: {filename}

Content:
{text_content[:8000]}  # Limit context for cost

Extract: document type, topics, programming languages/frameworks, date references, key entities, summary, and technical level."""
        }],
        response_format=DocumentMetadata
    )

    return completion.choices[0].message.parsed
```

### 3. Storage Strategy

Add `metadata` JSONB column to both `documents` and `chunks` tables:
- **documents.metadata**: Document-level metadata (applies to whole file)
- **chunks.metadata**: Inherited from parent document + chunk-specific metadata (optional)

### 4. Retrieval Enhancement

Update `match_chunks` function to support metadata filters:
```sql
CREATE OR REPLACE FUNCTION match_chunks(
    query_embedding vector(1536),
    match_threshold float,
    match_count int,
    user_id_param uuid,
    metadata_filters jsonb DEFAULT NULL
)
RETURNS TABLE (
    id uuid,
    document_id uuid,
    content text,
    metadata jsonb,
    similarity float
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        c.id,
        c.document_id,
        c.content,
        c.metadata,
        1 - (c.embedding <=> query_embedding) AS similarity
    FROM chunks c
    WHERE
        c.user_id = user_id_param
        AND 1 - (c.embedding <=> query_embedding) > match_threshold
        -- Apply metadata filters if provided
        AND (
            metadata_filters IS NULL
            OR c.metadata @> metadata_filters
        )
    ORDER BY c.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;
```

### 5. Metadata-Aware Retrieval Tool

Update the `search_documents` tool to accept optional metadata filters:
```python
{
    "type": "function",
    "function": {
        "name": "search_documents",
        "description": "Search user's documents by query and optional metadata filters",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "Search query"
                },
                "metadata_filters": {
                    "type": "object",
                    "description": "Optional filters: {\"document_type\": \"tutorial\", \"topics\": [\"Python\"]}",
                    "properties": {
                        "document_type": {"type": "string"},
                        "topics": {"type": "array", "items": {"type": "string"}},
                        "programming_languages": {"type": "array", "items": {"type": "string"}},
                        "technical_level": {"type": "string"}
                    }
                }
            },
            "required": ["query"]
        }
    }
}
```

---

## Database Changes

### Migration: Add metadata columns

**File:** `backend/migrations/003_add_metadata.sql`

```sql
-- Add metadata column to documents table
ALTER TABLE documents
ADD COLUMN metadata JSONB DEFAULT '{}'::jsonb;

-- Add metadata column to chunks table
ALTER TABLE chunks
ADD COLUMN metadata JSONB DEFAULT '{}'::jsonb;

-- Create GIN index for fast JSONB queries on chunks
CREATE INDEX idx_chunks_metadata ON chunks USING GIN (metadata);

-- Create GIN index for fast JSONB queries on documents
CREATE INDEX idx_documents_metadata ON documents USING GIN (metadata);

-- Update match_chunks function to support metadata filtering
CREATE OR REPLACE FUNCTION match_chunks(
    query_embedding vector(1536),
    match_threshold float,
    match_count int,
    user_id_param uuid,
    metadata_filters jsonb DEFAULT NULL
)
RETURNS TABLE (
    id uuid,
    document_id uuid,
    content text,
    metadata jsonb,
    similarity float
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        c.id,
        c.document_id,
        c.content,
        c.metadata,
        1 - (c.embedding <=> query_embedding) AS similarity
    FROM chunks c
    WHERE
        c.user_id = user_id_param
        AND 1 - (c.embedding <=> query_embedding) > match_threshold
        AND (
            metadata_filters IS NULL
            OR c.metadata @> metadata_filters
        )
    ORDER BY c.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;
```

---

## Task 1: Database Migration

**File:** `backend/migrations/003_add_metadata.sql`

Create migration file with:
- Add `metadata JSONB` column to `documents` table
- Add `metadata JSONB` column to `chunks` table
- Create GIN indexes on both for fast JSONB queries
- Update `match_chunks` function to accept `metadata_filters` parameter

**Validation:**
- Run `supabase db push`
- Verify columns exist: `SELECT metadata FROM documents LIMIT 1;`
- Verify indexes: Check Supabase dashboard or `\di` in psql
- Test function signature: `SELECT * FROM match_chunks('{...}'::vector, 0.5, 10, '...'::uuid, '{"document_type": "tutorial"}'::jsonb);`

---

## Task 2: Define Metadata Schema

**File:** `backend/app/models/schemas.py`

Add `DocumentMetadata` Pydantic model with fields:
- `document_type`: Literal type for categories
- `topics`: List of main subjects (max 5)
- `programming_languages`: List of languages mentioned
- `frameworks_tools`: List of frameworks/tools (max 5)
- `date_references`: Optional string for temporal info
- `key_entities`: Important names/companies (max 5)
- `summary`: One-sentence summary (max 200 chars)
- `technical_level`: Beginner/intermediate/advanced/expert

Update `DocumentResponse` to include metadata:
```python
class DocumentResponse(BaseModel):
    # ... existing fields
    metadata: dict | None = None  # Add this
```

**Validation:**
- Import works: `from app.models.schemas import DocumentMetadata`
- Pydantic validation works: `DocumentMetadata(document_type="tutorial", topics=["Python"], summary="Test")`
- Extra fields rejected: Try passing invalid `document_type`, should raise validation error

---

## Task 3: Create Metadata Extraction Service

**File:** `backend/app/services/metadata_service.py` (new file)

Implement `extract_metadata()` function:
1. Takes `text_content` and `filename` as input
2. Uses OpenAI client with structured outputs (`beta.chat.completions.parse`)
3. Sends first 8000 characters of content to LLM (cost optimization)
4. Returns parsed `DocumentMetadata` object
5. Includes error handling for extraction failures

**Important:**
- Use `settings.llm_base_url` and `settings.llm_api_key` (OpenRouter)
- Use `settings.llm_model` for model selection
- Limit text to 8000 characters to avoid excessive costs
- Handle cases where extraction fails (return default metadata)

**Validation:**
- Import works without errors
- Test with sample text: `metadata = await extract_metadata("This is a Python tutorial...", "python_guide.md")`
- Verify structured output: `metadata.document_type == "tutorial"`
- Verify all fields present and valid

---

## Task 4: Integrate Metadata Extraction into Ingestion

**File:** `backend/app/services/ingestion_service.py`

Update `ingest_document()` to:
1. After reading file content, before chunking:
   ```python
   # Extract metadata from full document
   from app.services.metadata_service import extract_metadata

   document_metadata = await extract_metadata(text_content, filename)
   ```

2. Store metadata in `documents` table:
   ```python
   supabase.table("documents").update({
       "metadata": document_metadata.model_dump()
   }).eq("id", document_id).execute()
   ```

3. Inherit metadata to chunks:
   ```python
   # When creating chunks, include document metadata
   chunk_data = {
       "document_id": document_id,
       "content": chunk_text,
       "embedding": embedding,
       "chunk_index": i,
       "metadata": document_metadata.model_dump(),  # Inherit from doc
       "user_id": user_id
   }
   ```

**Validation:**
- Upload a document via POST /documents
- Check `documents` table: `metadata` field populated with structured data
- Check `chunks` table: each chunk has inherited `metadata` from parent document
- Verify metadata structure matches `DocumentMetadata` schema

---

## Task 5: Update Retrieval Service

**File:** `backend/app/services/retrieval_service.py`

Update `search_documents()` to accept `metadata_filters`:
```python
async def search_documents(
    query: str,
    user_id: str,
    top_k: int = 5,
    metadata_filters: dict | None = None  # Add this parameter
) -> list[dict]:
    """Search documents with optional metadata filtering."""

    # ... existing embedding logic

    # Call match_chunks with metadata_filters
    result = supabase.rpc(
        'match_chunks',
        {
            'query_embedding': query_embedding,
            'match_threshold': 0.7,
            'match_count': top_k,
            'user_id_param': user_id,
            'metadata_filters': metadata_filters  # Pass filters
        }
    ).execute()

    # ... rest of function
```

**Validation:**
- Test without filters: `search_documents("Python tutorial", user_id)` - works as before
- Test with filters: `search_documents("Python tutorial", user_id, metadata_filters={"document_type": "tutorial"})` - returns only tutorials
- Test with array filter: `search_documents("code", user_id, metadata_filters={"programming_languages": ["Python"]})` - returns only Python docs

---

## Task 6: Update RAG Tool Definition

**File:** `backend/app/services/tool_executor.py`

Update `RAG_TOOLS` list to include `metadata_filters` parameter:
```python
RAG_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "search_documents",
            "description": """Search user's uploaded documents by semantic similarity.

            Supports metadata filtering:
            - document_type: 'tutorial', 'guide', 'reference', 'blog_post', 'research_paper', 'documentation', 'report', 'other'
            - topics: array of topic strings
            - programming_languages: array of language names
            - frameworks_tools: array of framework/tool names
            - technical_level: 'beginner', 'intermediate', 'advanced', 'expert'

            Examples:
            - Find Python tutorials: metadata_filters={"document_type": "tutorial", "programming_languages": ["Python"]}
            - Find beginner guides: metadata_filters={"document_type": "guide", "technical_level": "beginner"}
            - Find React documentation: metadata_filters={"frameworks_tools": ["React"]}
            """,
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search query"
                    },
                    "metadata_filters": {
                        "type": "object",
                        "description": "Optional JSONB metadata filters to narrow search",
                        "properties": {
                            "document_type": {
                                "type": "string",
                                "enum": ["tutorial", "guide", "reference", "blog_post", "research_paper", "documentation", "report", "other"]
                            },
                            "topics": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "programming_languages": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "frameworks_tools": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "technical_level": {
                                "type": "string",
                                "enum": ["beginner", "intermediate", "advanced", "expert"]
                            }
                        }
                    }
                },
                "required": ["query"]
            }
        }
    }
]
```

Update `execute_tool()` to pass `metadata_filters` to `search_documents()`:
```python
async def execute_tool(tool_name: str, arguments: dict, user_id: str) -> dict:
    """Execute a tool by name with provided arguments."""

    if tool_name == "search_documents":
        query = arguments.get("query")
        metadata_filters = arguments.get("metadata_filters")  # Extract filters

        results = await search_documents(
            query=query,
            user_id=user_id,
            metadata_filters=metadata_filters  # Pass to search
        )

        # ... rest of function
```

**Validation:**
- LLM can now see metadata filtering capability in tool description
- Test query: "Find Python tutorials for beginners" - LLM should call tool with appropriate filters
- Verify tool arguments in LangSmith traces include `metadata_filters` when appropriate

---

## Task 7: Add Metadata Display in UI

**File:** `frontend/src/components/DocumentList.tsx`

Add metadata display to document cards:
```tsx
{doc.metadata && (
  <div className="mt-2 flex flex-wrap gap-2 text-xs">
    {doc.metadata.document_type && (
      <span className="px-2 py-1 bg-blue-100 text-blue-800 rounded">
        {doc.metadata.document_type}
      </span>
    )}
    {doc.metadata.topics?.map((topic: string) => (
      <span key={topic} className="px-2 py-1 bg-green-100 text-green-800 rounded">
        {topic}
      </span>
    ))}
    {doc.metadata.programming_languages?.map((lang: string) => (
      <span key={lang} className="px-2 py-1 bg-purple-100 text-purple-800 rounded">
        {lang}
      </span>
    )}
    {doc.metadata.technical_level && (
      <span className="px-2 py-1 bg-orange-100 text-orange-800 rounded">
        {doc.metadata.technical_level}
      </span>
    )}
  </div>
)}
```

Optionally add expandable metadata panel:
```tsx
{showMetadata && doc.metadata && (
  <div className="mt-2 p-3 bg-gray-50 rounded text-sm">
    <div className="font-semibold mb-2">Extracted Metadata</div>
    <div className="space-y-1">
      {doc.metadata.summary && (
        <p className="italic">"{doc.metadata.summary}"</p>
      )}
      {/* ... display all metadata fields */}
    </div>
  </div>
)}
```

**Validation:**
- Upload a document, wait for processing
- Metadata badges appear on document card
- Metadata is color-coded and readable
- Clicking "Show metadata" expands full details

---

## Task 8: Backfill Existing Documents

**File:** `backend/app/scripts/backfill_metadata.py` (new file)

Create script to extract metadata for documents that don't have it:
```python
"""Backfill metadata for existing documents."""
import asyncio
from supabase import create_client
from app.config import get_settings
from app.services.metadata_service import extract_metadata

settings = get_settings()
supabase = create_client(settings.supabase_url, settings.supabase_service_role_key)

async def backfill_metadata():
    """Extract metadata for documents missing it."""

    # Get documents without metadata (metadata is NULL or empty object)
    docs = supabase.table("documents").select("*").or_(
        "metadata.is.null,metadata.eq.{}"
    ).eq("status", "completed").execute()

    print(f"Found {len(docs.data)} documents without metadata")

    for doc in docs.data:
        try:
            # Download file from storage
            file_data = supabase.storage.from_("documents").download(doc["storage_path"])
            text_content = file_data.decode('utf-8')

            # Extract metadata
            metadata = await extract_metadata(text_content, doc["filename"])

            # Update document
            supabase.table("documents").update({
                "metadata": metadata.model_dump()
            }).eq("id", doc["id"]).execute()

            # Update all chunks with inherited metadata
            supabase.table("chunks").update({
                "metadata": metadata.model_dump()
            }).eq("document_id", doc["id"]).execute()

            print(f"✓ Updated {doc['filename']}")

        except Exception as e:
            print(f"✗ Failed {doc['filename']}: {e}")

    print("Backfill complete!")

if __name__ == "__main__":
    asyncio.run(backfill_metadata())
```

**Validation:**
- Run script: `cd backend && python -m app.scripts.backfill_metadata`
- All existing documents now have `metadata` populated
- All existing chunks inherit metadata from parent document
- Check sample document in Supabase dashboard - metadata field is valid JSON matching schema

---

## Task 9: Update Validation Suite

**File:** `.agent/validation/full-suite.md`

Add new tests:

### API-41: Upload Document with Metadata Extraction
**Steps:**
1. Upload a new document (e.g., Python tutorial)
2. Wait for status to be "completed"
3. Fetch document details: GET /documents/{id}
4. Inspect `metadata` field

**Acceptance:**
- Document has `metadata` object populated
- `metadata.document_type` is set (not null)
- `metadata.topics` is a non-empty array
- `metadata.summary` is a non-empty string
- All metadata fields follow `DocumentMetadata` schema

### API-42: Search with Metadata Filters
**Steps:**
1. Upload two documents: one Python tutorial, one JavaScript guide
2. Wait for both to complete
3. Search without filters: POST /chat (via tool call) with query "programming tutorial"
4. Search with filter: POST /chat with query "programming tutorial" + metadata_filters={"programming_languages": ["Python"]}
5. Compare results

**Acceptance:**
- Without filters: returns chunks from both documents
- With Python filter: returns only Python document chunks
- With JavaScript filter: returns only JavaScript document chunks
- Non-matching filter returns empty results

### API-43: Metadata Inheritance to Chunks
**Steps:**
1. Upload a document
2. Wait for completion
3. Query `chunks` table for all chunks belonging to this document
4. Inspect `metadata` field on each chunk

**Acceptance:**
- All chunks have `metadata` field populated
- Chunk metadata matches parent document metadata
- Metadata is identical across all chunks from same document

### E2E-28: Metadata Display in UI
**Steps:**
1. Sign in as test@test.com
2. Navigate to Documents page
3. Upload a technical document (Python code, tutorial, etc.)
4. Wait for status to show "completed"
5. Observe document card in list

**Acceptance:**
- Metadata badges appear (document type, topics, languages, etc.)
- Badges are color-coded and readable
- Can click to expand full metadata panel
- Summary text displays correctly

---

## Edge Cases to Handle

1. **Extraction fails:** Default to empty metadata `{}`, mark document as completed anyway
2. **Invalid metadata returned:** Pydantic validation catches it, use default metadata
3. **Very long documents:** Truncate to 8000 chars before extraction (cost optimization)
4. **Documents with no clear type:** LLM should return `"other"` for `document_type`
5. **Non-English content:** Extraction should still work, topics/entities in source language
6. **Binary or non-text files:** Skip metadata extraction (handled in Task 3 error handling)
7. **Metadata filtering with no results:** Return empty list, not an error

---

## Cost Considerations

Metadata extraction adds LLM calls to ingestion:
- ~8000 chars per document = ~2000 tokens input
- Structured output = ~200 tokens output
- Cost per document: ~$0.002 with OpenRouter (varies by model)
- For 1000 documents: ~$2

**Optimization strategies:**
- Truncate content to first 8000 chars (usually enough for classification)
- Use cheaper models for extraction (e.g., GPT-4o-mini vs GPT-4)
- Cache extraction results (already done via content_hash)
- Consider batching small documents

---

## Verification Checklist

- [ ] Migration applied successfully (metadata columns + indexes)
- [ ] `DocumentMetadata` schema defined with all fields
- [ ] `extract_metadata()` service works with structured outputs
- [ ] Ingestion pipeline extracts and stores metadata
- [ ] Chunks inherit metadata from parent document
- [ ] `search_documents()` accepts `metadata_filters` parameter
- [ ] `match_chunks()` function applies JSONB filters correctly
- [ ] RAG tool definition includes metadata filter parameters
- [ ] LLM successfully calls tool with filters when appropriate
- [ ] UI displays metadata badges on document cards
- [ ] Backfill script works for existing documents
- [ ] Validation suite tests pass (API-41, API-42, API-43, E2E-28)
- [ ] Test query: "Find Python tutorials" → LLM uses metadata filter
- [ ] Test query: "Show me beginner guides" → LLM filters by technical_level
- [ ] Update PROGRESS.md with Module 4 completion

---

## Files Modified

| File | Change |
|------|--------|
| `backend/migrations/003_add_metadata.sql` | New migration for metadata columns, indexes, updated match_chunks |
| `backend/app/models/schemas.py` | Add DocumentMetadata schema, update DocumentResponse |
| `backend/app/services/metadata_service.py` | New service for LLM-powered metadata extraction |
| `backend/app/services/ingestion_service.py` | Integrate metadata extraction into ingestion pipeline |
| `backend/app/services/retrieval_service.py` | Add metadata_filters parameter to search_documents |
| `backend/app/services/tool_executor.py` | Update RAG_TOOLS definition with metadata filters, pass filters in execute_tool |
| `frontend/src/components/DocumentList.tsx` | Display metadata badges and expandable panel |
| `backend/app/scripts/backfill_metadata.py` | New script to backfill existing documents |
| `.agent/validation/full-suite.md` | Add 4 new tests (3 API, 1 E2E) |
| `PROGRESS.md` | Mark Module 4 as completed |

---

## Testing Strategy

### Unit Tests (Manual)
1. Test metadata extraction with sample text
2. Test Pydantic validation with invalid data
3. Test metadata filtering in database queries

### Integration Tests (API)
1. API-41: Upload → verify metadata populated
2. API-42: Search with filters → verify filtered results
3. API-43: Check chunk metadata inheritance

### E2E Tests (Browser)
1. E2E-28: Upload document → verify metadata display in UI

### Acceptance Tests
1. Upload Python tutorial → metadata shows `document_type: "tutorial"`, `programming_languages: ["Python"]`
2. Upload financial report → metadata shows `document_type: "report"`, relevant topics
3. Query "Find Python tutorials" → LLM applies filter, returns only tutorials
4. Query "Show beginner guides" → LLM applies technical_level filter

---

## Complexity Assessment

⚠️ **Medium** - This plan has moderate complexity:

**What makes it Medium:**
- LLM integration for structured extraction (new pattern)
- JSONB filtering in Postgres (new database feature)
- Schema design requires thought (what metadata to extract)
- Tool definition needs clear description for LLM understanding
- UI changes for metadata display
- Backfill script for existing documents

**Why it's still feasible:**
- Clear Pydantic schema for structured outputs
- Well-defined database changes (JSONB + GIN indexes)
- Existing retrieval infrastructure (just add filters)
- No breaking changes to existing functionality
- Easy to validate (upload file, check metadata, test filters)

**Risks:**
- LLM extraction might be unreliable (mitigation: default to empty metadata)
- Cost of extraction on every upload (mitigation: truncate to 8000 chars)
- JSONB filtering performance (mitigation: GIN indexes)

An agent should be able to complete this in 1-2 passes with some iteration on the metadata schema and tool description.

---

## Success Criteria

Module 4 is complete when:
1. Documents have structured metadata extracted on upload
2. Metadata follows defined schema (document_type, topics, languages, etc.)
3. Chunks inherit metadata from parent document
4. Retrieval supports metadata filtering via JSONB queries
5. LLM tool has updated description with filter parameters
6. LLM successfully applies filters when queries warrant it
7. UI displays metadata badges on document cards
8. All validation tests pass (API-41, API-42, API-43, E2E-28)
9. Query "Find Python tutorials for beginners" returns filtered results
10. Existing documents can be backfilled with metadata

**Next Module:** Module 5 - Multi-Format Support (PDF/DOCX/HTML parsing)
