# Plan: Module 3 - Record Manager

**Complexity:** ✅ Simple

## Summary

Implement content hashing and deduplication to prevent duplicate documents. When a file is uploaded, hash its content and check if it already exists. If the content is identical, skip processing. If the content changed, update the existing document and re-process chunks. This prevents duplicate embeddings and wasted processing.

**Learning Goals:**
- Why naive ingestion creates duplicates
- Content hashing for change detection
- Incremental updates vs full re-ingestion
- Data consistency in RAG systems

---

## Current Problem

The existing ingestion pipeline has no deduplication:
1. User uploads `report.pdf`
2. System creates document record, chunks it, embeds chunks → 50 chunks in `chunks` table
3. User uploads `report.pdf` again (same file)
4. System creates a NEW document record, chunks it again → 50 MORE chunks (100 total)
5. RAG retrieval now returns duplicate results

**Impact:**
- Storage waste
- Duplicate search results
- Slower retrieval
- Poor user experience

---

## Solution Design

### 1. Content Hashing

Calculate SHA-256 hash of file contents on upload:
```python
import hashlib

def hash_file_content(file_bytes: bytes) -> str:
    """Return SHA-256 hash of file content as hex string."""
    return hashlib.sha256(file_bytes).hexdigest()
```

### 2. Deduplication Logic

Before processing a new upload:
1. Hash the file content
2. Query `documents` table for existing document with same `content_hash` AND `user_id`
3. If found:
   - **Same content:** Return existing document, skip processing
   - **Different filename:** Update filename, return existing document
4. If not found: Create new document, proceed with ingestion

### 3. Update Detection

When user re-uploads a file with the same name but different content:
1. Query by `filename` AND `user_id`
2. Compare `content_hash`
3. If different:
   - Delete old chunks (cascade from document)
   - Update document record with new hash, reset status to "processing"
   - Re-run ingestion pipeline

---

## Database Changes

### Migration: Add content_hash column

**File:** `backend/migrations/002_add_content_hash.sql`

```sql
-- Add content_hash column to documents table
ALTER TABLE documents
ADD COLUMN content_hash TEXT;

-- Create index for fast duplicate lookups
CREATE INDEX idx_documents_user_hash
ON documents(user_id, content_hash);

-- Create index for filename lookups (for update detection)
CREATE INDEX idx_documents_user_filename
ON documents(user_id, filename);
```

---

## Task 1: Database Migration

**File:** `backend/migrations/002_add_content_hash.sql`

Create migration file with:
- Add `content_hash TEXT` column to `documents`
- Index on `(user_id, content_hash)` for duplicate detection
- Index on `(user_id, filename)` for update detection

**Validation:**
- Run `supabase db push`
- Verify column exists: `SELECT content_hash FROM documents LIMIT 1;`
- Verify indexes: `\di` in psql or check Supabase dashboard

---

## Task 2: Add Hashing Utility

**File:** `backend/app/services/ingestion_service.py`

Add at top of file:
```python
import hashlib

def hash_file_content(file_bytes: bytes) -> str:
    """Calculate SHA-256 hash of file content."""
    return hashlib.sha256(file_bytes).hexdigest()
```

**Validation:**
- Import works without errors
- Test: `hash_file_content(b"test")` returns consistent hash

---

## Task 3: Implement Duplicate Detection

**File:** `backend/app/routers/documents.py`

Update the `upload_document` endpoint:

**Before ingestion, add:**
1. Read file bytes into memory: `file_bytes = await file.read()`
2. Calculate hash: `content_hash = hash_file_content(file_bytes)`
3. Check for duplicate:
   ```python
   # Check if document with same content already exists
   existing = supabase.table("documents").select("*").eq(
       "user_id", current_user.id
   ).eq("content_hash", content_hash).execute()

   if existing.data:
       # Exact duplicate - return existing document
       return existing.data[0]
   ```
4. Check for update (same filename, different content):
   ```python
   # Check if filename exists with different content
   existing_by_name = supabase.table("documents").select("*").eq(
       "user_id", current_user.id
   ).eq("filename", file.filename).execute()

   if existing_by_name.data:
       old_doc = existing_by_name.data[0]
       if old_doc["content_hash"] != content_hash:
           # Content changed - delete old chunks
           supabase.table("chunks").delete().eq("document_id", old_doc["id"]).execute()

           # Update document record
           supabase.table("documents").update({
               "content_hash": content_hash,
               "status": "processing",
               "chunk_count": 0,
               "error_message": None,
               "updated_at": datetime.utcnow().isoformat(),
           }).eq("id", old_doc["id"]).execute()

           # Use existing document ID for re-ingestion
           document_id = old_doc["id"]
       else:
           # Same content, same filename - return existing
           return old_doc
   ```
5. If new file, create document record with `content_hash`
6. Pass `file_bytes` to ingestion (already in memory, don't re-read)

**Validation:**
- Upload a file → document created with content_hash populated
- Upload same file again → returns existing document, no new chunks created
- Upload file with same name but different content → chunks replaced, document updated

---

## Task 4: Update Ingestion Service

**File:** `backend/app/services/ingestion_service.py`

Update `ingest_document` function signature to accept either file path OR bytes:
```python
async def ingest_document(
    document_id: str,
    file_path: str | None = None,
    file_bytes: bytes | None = None,
    user_id: str | None = None
) -> None:
    """Ingest document from file path or bytes."""
    if file_bytes:
        text_content = file_bytes.decode('utf-8')
    else:
        with open(file_path, 'rb') as f:
            text_content = f.read().decode('utf-8')

    # ... rest of ingestion logic
```

Update `documents.py` to pass `file_bytes` instead of `file_path` when available.

**Validation:**
- Ingestion works with `file_bytes` parameter
- No file system writes for duplicate detection path

---

## Task 5: Update Schema (Optional)

**File:** `backend/app/models/schemas.py`

Add `content_hash` to `DocumentResponse` if you want to expose it in API responses:
```python
class DocumentResponse(BaseModel):
    id: str
    user_id: str
    filename: str
    file_type: str
    file_size: int
    storage_path: str
    status: str
    error_message: str | None = None
    chunk_count: int = 0
    content_hash: str | None = None  # Add this
    created_at: datetime
    updated_at: datetime
```

**Validation:**
- API responses include `content_hash` field

---

## Task 6: Backfill Existing Documents

**File:** `backend/app/scripts/backfill_hashes.py` (new file)

Create a script to calculate hashes for existing documents:
```python
"""Backfill content_hash for existing documents."""
import hashlib
from supabase import create_client
from app.config import get_settings

settings = get_settings()
supabase = create_client(settings.supabase_url, settings.supabase_service_role_key)

def backfill_hashes():
    # Get all documents without content_hash
    docs = supabase.table("documents").select("*").is_("content_hash", None).execute()

    for doc in docs.data:
        try:
            # Download file from storage
            file_data = supabase.storage.from_("documents").download(doc["storage_path"])

            # Calculate hash
            content_hash = hashlib.sha256(file_data).hexdigest()

            # Update document
            supabase.table("documents").update({
                "content_hash": content_hash
            }).eq("id", doc["id"]).execute()

            print(f"✓ Updated {doc['filename']}")
        except Exception as e:
            print(f"✗ Failed {doc['filename']}: {e}")

if __name__ == "__main__":
    backfill_hashes()
```

**Validation:**
- Run script: `cd backend && python -m app.scripts.backfill_hashes`
- All existing documents have `content_hash` populated

---

## Task 7: Update Validation Suite

**File:** `.agent/validation/full-suite.md`

Add new tests:

### API-37: Upload Duplicate File (Exact)
**Steps:**
1. Upload `test_document.txt` via POST /documents
2. Note the document ID and chunk count
3. Upload the same file again (exact same content)
4. Check response

**Acceptance:**
- Second upload returns the SAME document ID
- No new chunks created (chunk count unchanged)
- Total document count is 1, not 2

### API-38: Upload Modified File
**Steps:**
1. Upload `test_document.txt` via POST /documents
2. Note the document ID
3. Modify the file content (add a line)
4. Upload modified file with same filename
5. Check document ID and chunk count

**Acceptance:**
- Same document ID returned
- Old chunks deleted, new chunks created
- Document status changes to "processing" then "completed"
- `content_hash` updated in database

### API-39: Upload Same Content, Different Filename
**Steps:**
1. Upload `test_document.txt`
2. Copy content to `test_document_copy.txt`
3. Upload `test_document_copy.txt`

**Acceptance:**
- Returns existing document (by content_hash)
- No new chunks created
- Only one document record exists

### E2E-24: Duplicate Upload Behavior
**Steps:**
1. Sign in as test@test.com
2. Navigate to Documents page
3. Upload `test_rag_document.txt`
4. Wait for "completed" status
5. Upload the same file again
6. Observe behavior

**Acceptance:**
- File re-uploaded successfully
- No duplicate shown in document list
- No error or loading state (immediate return)
- Document still shows "completed"

---

## Edge Cases to Handle

1. **Filename renamed, same content:** Return existing document by hash
2. **Content changed, same filename:** Delete chunks, re-ingest
3. **Hash collision (theoretical):** Extremely unlikely with SHA-256, no handling needed
4. **Concurrent uploads of same file:** Database uniqueness constraint on (user_id, content_hash) would catch it
5. **Partial file upload (interrupted):** Status remains "processing", can be retried

---

## Verification Checklist

- [ ] Migration applied successfully
- [ ] Upload new file → `content_hash` populated in DB
- [ ] Upload same file twice → returns existing document, no duplicate chunks
- [ ] Upload modified file → old chunks deleted, new chunks created
- [ ] Upload same content with different filename → returns existing document
- [ ] Backfill script runs successfully for existing documents
- [ ] Validation suite tests pass (API-37, API-38, API-39, E2E-24)
- [ ] Update PROGRESS.md with Module 3 completion

---

## Files Modified

| File | Change |
|------|--------|
| `backend/migrations/002_add_content_hash.sql` | New migration for content_hash column and indexes |
| `backend/app/routers/documents.py` | Add duplicate detection logic in upload_document |
| `backend/app/services/ingestion_service.py` | Add hash_file_content, support file_bytes parameter |
| `backend/app/models/schemas.py` | Add content_hash field to DocumentResponse (optional) |
| `backend/app/scripts/backfill_hashes.py` | New script to backfill existing documents |
| `.agent/validation/full-suite.md` | Add 4 new tests (3 API, 1 E2E) |
| `PROGRESS.md` | Mark Module 3 as completed |

---

## Complexity Assessment

✅ **Simple** - This plan is single-pass executable:
- Straightforward hashing logic
- Clear database changes (one column, two indexes)
- Well-defined deduplication rules
- Minimal files affected
- Low risk (no breaking changes to existing functionality)
- Easy to validate (upload same file, check for duplicate)

An agent should be able to complete this in one go without iteration.
