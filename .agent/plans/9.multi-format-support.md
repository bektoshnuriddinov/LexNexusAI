# Plan: Module 6 - Multi-Format Support

**Complexity:** ‚ö†Ô∏è Medium

## Summary

Expand document ingestion to support PDF, DOCX, and HTML files in addition to plain text and Markdown. Implement proper text extraction for each format, handle parsing errors gracefully, and ensure cascade deletes clean up all associated data. This enables users to upload a wider variety of document types for RAG.

**Learning Goals:**
- Document parsing libraries and their quirks
- Format-specific text extraction challenges
- Error handling for corrupt/unsupported files
- Database cascade deletes for data integrity

---

## Current Problem

The existing ingestion pipeline only supports `.txt` and `.md` files:
1. User tries to upload `research_paper.pdf` ‚Üí rejected by file type validation
2. User tries to upload `company_policy.docx` ‚Üí rejected by file type validation
3. User tries to upload `documentation.html` ‚Üí rejected by file type validation

**Impact:**
- Limited usefulness - most real-world documents are PDFs or DOCX
- Manual conversion required (user must extract text first)
- Poor user experience
- Can't ingest many common document types

---

## Solution Design

### 1. Supported Formats

| Format | Extension | MIME Type | Library |
|--------|-----------|-----------|---------|
| Plain Text | .txt | text/plain | Built-in (decode) |
| Markdown | .md | text/markdown | Built-in (decode) |
| PDF | .pdf | application/pdf | **pypdf** (PyPDF2 successor) |
| Word Documents | .docx | application/vnd.openxmlformats-officedocument.wordprocessingml.document | **python-docx** |
| HTML | .html, .htm | text/html | **beautifulsoup4** + html2text |

### 2. Text Extraction Strategy

```python
# backend/app/services/extraction_service.py

from pypdf import PdfReader
from docx import Document
from bs4 import BeautifulSoup
import html2text

def extract_text_from_pdf(file_bytes: bytes) -> str:
    """Extract text from PDF using pypdf."""
    import io
    pdf_reader = PdfReader(io.BytesIO(file_bytes))

    text_parts = []
    for page in pdf_reader.pages:
        text_parts.append(page.extract_text())

    return "\n\n".join(text_parts)


def extract_text_from_docx(file_bytes: bytes) -> str:
    """Extract text from DOCX using python-docx."""
    import io
    doc = Document(io.BytesIO(file_bytes))

    text_parts = []
    for paragraph in doc.paragraphs:
        text_parts.append(paragraph.text)

    return "\n\n".join(text_parts)


def extract_text_from_html(file_bytes: bytes) -> str:
    """Extract text from HTML using BeautifulSoup and html2text."""
    html_content = file_bytes.decode('utf-8', errors='ignore')

    # Option 1: Clean HTML to Markdown
    h = html2text.HTML2Text()
    h.ignore_links = False
    h.ignore_images = True
    return h.handle(html_content)

    # Option 2: Extract plain text only
    # soup = BeautifulSoup(html_content, 'html.parser')
    # return soup.get_text(separator='\n\n', strip=True)
```

### 3. File Type Detection

Update allowed types in documents router:
```python
ALLOWED_TYPES = {
    "text/plain": [".txt"],
    "text/markdown": [".md"],
    "application/pdf": [".pdf"],
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": [".docx"],
    "text/html": [".html", ".htm"],
    "application/octet-stream": None,  # Fallback, validate by extension
}

ALLOWED_EXTENSIONS = {".txt", ".md", ".pdf", ".docx", ".html", ".htm"}
```

### 4. Cascade Deletes

Ensure database schema has proper foreign key constraints:
```sql
-- Chunks table already has ON DELETE CASCADE
CREATE TABLE chunks (
    ...
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    ...
);
```

When deleting a document:
1. Delete from storage bucket
2. Delete document record ‚Üí chunks automatically deleted via CASCADE
3. No orphaned chunks remain

---

## Database Changes

### Migration: Verify cascade deletes

**File:** `backend/migrations/004_verify_cascade_deletes.sql`

```sql
-- Verify that chunks have proper cascade delete
-- This should already exist from earlier migrations, but verify

DO $$
BEGIN
    IF NOT EXISTS (
        SELECT 1
        FROM information_schema.table_constraints tc
        JOIN information_schema.key_column_usage kcu
            ON tc.constraint_name = kcu.constraint_name
        WHERE tc.table_name = 'chunks'
            AND tc.constraint_type = 'FOREIGN KEY'
            AND kcu.column_name = 'document_id'
    ) THEN
        -- Add foreign key if missing
        ALTER TABLE chunks
        ADD CONSTRAINT chunks_document_id_fkey
        FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE;
    END IF;
END $$;

-- Add comment for documentation
COMMENT ON CONSTRAINT chunks_document_id_fkey ON chunks IS
'Cascade delete: when a document is deleted, all its chunks are automatically removed';
```

**Note:** This migration is mostly verification. The CASCADE should already exist from initial schema, but we verify it explicitly.

---

## Task 1: Install Document Parsing Libraries

**File:** `backend/requirements.txt`

Add new dependencies:
```txt
pypdf==4.0.1          # PDF parsing (PyPDF2 successor)
python-docx==1.1.0    # DOCX parsing
beautifulsoup4==4.12.3  # HTML parsing
html2text==2024.2.26  # HTML to Markdown conversion
```

**Validation:**
```bash
cd backend
source venv/bin/activate  # or venv/Scripts/activate on Windows
pip install -r requirements.txt
python -c "import pypdf, docx, bs4, html2text; print('‚úì All libraries installed')"
```

---

## Task 2: Create Text Extraction Service

**File:** `backend/app/services/extraction_service.py` (new file)

Implement extraction functions for each format:
1. `extract_text_from_pdf(file_bytes: bytes) -> str`
2. `extract_text_from_docx(file_bytes: bytes) -> str`
3. `extract_text_from_html(file_bytes: bytes) -> str`
4. `extract_text(file_bytes: bytes, file_type: str) -> str` - dispatcher function

**Key features:**
- Handle corrupt/unreadable files gracefully
- Log extraction errors
- Return empty string or raise specific exceptions
- Preserve structure where possible (paragraphs, headings)

**Error handling:**
```python
try:
    text = extract_text_from_pdf(file_bytes)
    if not text.strip():
        raise ValueError("No text extracted from PDF (might be scanned image)")
except Exception as e:
    logger.error(f"PDF extraction failed: {e}")
    raise ValueError(f"Failed to extract text from PDF: {e}")
```

**Validation:**
- Test with sample PDF (text-based, not scanned)
- Test with sample DOCX
- Test with sample HTML
- Test with corrupted files (should raise clear errors)

---

## Task 3: Update File Type Validation

**File:** `backend/app/routers/documents.py`

Update `ALLOWED_TYPES` and `ALLOWED_EXTENSIONS`:
```python
ALLOWED_TYPES = {
    "text/plain": [".txt"],
    "text/markdown": [".md"],
    "application/pdf": [".pdf"],
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": [".docx"],
    "text/html": [".html", ".htm"],
    "application/octet-stream": None,  # Fallback - validate by extension
}

ALLOWED_EXTENSIONS = {".txt", ".md", ".pdf", ".docx", ".html", ".htm"}

MAX_FILE_SIZE = 50 * 1024 * 1024  # Increase to 50 MB for PDFs
```

Update validation logic to handle multi-extension MIME types:
```python
# Validate file extension
filename = file.filename or "unknown"
ext = "." + filename.rsplit(".", 1)[-1].lower() if "." in filename else ""

if ext not in ALLOWED_EXTENSIONS:
    raise HTTPException(
        status_code=status.HTTP_400_BAD_REQUEST,
        detail=f"Unsupported file type. Allowed: {', '.join(sorted(ALLOWED_EXTENSIONS))}"
    )
```

**Validation:**
- Upload .pdf file ‚Üí accepted
- Upload .docx file ‚Üí accepted
- Upload .html file ‚Üí accepted
- Upload .py file ‚Üí rejected with clear message
- Upload .jpg file ‚Üí rejected

---

## Task 4: Update Ingestion Service

**File:** `backend/app/services/ingestion_service.py`

Replace `extract_text()` function with call to extraction service:
```python
from app.services.extraction_service import extract_text as extract_text_from_file

async def process_document(document_id: str, user_id: str) -> None:
    """Process an uploaded document: extract text, chunk, embed, and store."""
    supabase = get_supabase_client()

    try:
        # ... existing code ...

        # Download file from storage
        storage_path = doc["storage_path"]
        file_bytes = supabase.storage.from_("documents").download(storage_path)

        # Extract text based on file type (now supports PDF, DOCX, HTML)
        text = extract_text_from_file(file_bytes, doc["file_type"])

        if not text.strip():
            raise ValueError(f"No text content extracted from {doc['file_type']} document")

        # ... rest of ingestion (metadata extraction, chunking, embedding) ...
```

Remove the old inline `extract_text()` function since it's now in extraction_service.py.

**Validation:**
- Upload PDF ‚Üí text extracted correctly
- Upload DOCX ‚Üí text extracted correctly
- Upload HTML ‚Üí text extracted and converted to readable format
- Check logs for extraction timing and any warnings

---

## Task 5: Add Sample Test Files

**File:** `.agent/validation/fixtures/` directory

Create sample files for testing:

**1. `test_document.pdf`**
Create a simple PDF with text:
```
# Python Programming Basics

This is a sample PDF document for testing PDF text extraction.

## Topics Covered
- Variables and data types
- Functions and methods
- Object-oriented programming

This document tests multi-format support in the RAG system.
```

**2. `test_document.docx`**
Create a Word document with similar content.

**3. `test_document.html`**
```html
<!DOCTYPE html>
<html>
<head><title>Python Programming Basics</title></head>
<body>
    <h1>Python Programming Basics</h1>
    <p>This is a sample HTML document for testing HTML text extraction.</p>

    <h2>Topics Covered</h2>
    <ul>
        <li>Variables and data types</li>
        <li>Functions and methods</li>
        <li>Object-oriented programming</li>
    </ul>

    <p>This document tests multi-format support in the RAG system.</p>
</body>
</html>
```

**Manual creation steps:**
- Use any PDF generator or LibreOffice to create test_document.pdf
- Use Microsoft Word or LibreOffice Writer to create test_document.docx
- Create test_document.html manually as shown above

**Validation:**
- All three files exist in fixtures directory
- Files are readable and not corrupt
- Content is similar across formats for comparison testing

---

## Task 6: Verify Cascade Deletes

**File:** `backend/migrations/004_verify_cascade_deletes.sql`

Create migration to verify/fix cascade delete constraints.

**Validation:**
```sql
-- Check constraint exists
SELECT tc.constraint_name, tc.table_name, kcu.column_name,
       ccu.table_name AS foreign_table_name,
       ccu.column_name AS foreign_column_name,
       rc.delete_rule
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
JOIN information_schema.referential_constraints AS rc
  ON rc.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
  AND tc.table_name='chunks'
  AND kcu.column_name='document_id';
```

Expected result: `delete_rule = 'CASCADE'`

**Test cascade delete:**
1. Upload a document
2. Verify chunks created
3. Delete document via API
4. Query chunks table ‚Üí should be 0 chunks for that document_id

---

## Task 7: Update Frontend File Type Display

**File:** `frontend/src/components/documents/DocumentList.tsx`

Add file type icons or indicators:
```tsx
function getFileIcon(fileType: string) {
  if (fileType.includes('pdf')) return 'üìÑ'
  if (fileType.includes('word') || fileType.includes('docx')) return 'üìù'
  if (fileType.includes('html')) return 'üåê'
  if (fileType.includes('markdown')) return 'üìã'
  return 'üìÑ'
}

// In document card:
<div className="flex items-center gap-2">
  <span className="text-lg">{getFileIcon(doc.file_type)}</span>
  <p className="text-sm font-medium truncate">{doc.filename}</p>
  <StatusBadge status={doc.status} />
</div>
```

**Validation:**
- Upload PDF ‚Üí shows üìÑ icon
- Upload DOCX ‚Üí shows üìù icon
- Upload HTML ‚Üí shows üåê icon
- Upload TXT ‚Üí shows üìÑ icon

---

## Task 8: Update Validation Suite

**File:** `.agent/validation/full-suite.md`

Add new tests:

### API-47: Upload PDF document
**Steps:**
```bash
# Upload PDF file
PDF_DOC=$(curl -s -X POST http://localhost:8000/documents/upload \
  -H "Authorization: Bearer $TOKEN1" \
  -F "file=@.agent/validation/fixtures/test_document.pdf" | jq -r '.id')

# Wait for processing
sleep 10

# Check status
curl -s http://localhost:8000/documents \
  -H "Authorization: Bearer $TOKEN1" | jq ".[] | select(.id==\"$PDF_DOC\") | {status, chunk_count, metadata}"
```
**Acceptance Criteria:**
- Upload succeeds (200 OK)
- Document status transitions to "completed"
- Chunks are created (chunk_count > 0)
- Metadata is extracted
- No extraction errors in logs

---

### API-48: Upload DOCX document
**Steps:**
```bash
# Upload DOCX file
DOCX_DOC=$(curl -s -X POST http://localhost:8000/documents/upload \
  -H "Authorization: Bearer $TOKEN1" \
  -F "file=@.agent/validation/fixtures/test_document.docx" | jq -r '.id')

# Wait for processing
sleep 10

# Check status
curl -s http://localhost:8000/documents \
  -H "Authorization: Bearer $TOKEN1" | jq ".[] | select(.id==\"$DOCX_DOC\") | {status, chunk_count}"
```
**Acceptance Criteria:**
- Upload succeeds
- Text extraction works correctly
- Chunks created successfully
- Status shows "completed"

---

### API-49: Upload HTML document
**Steps:**
```bash
# Upload HTML file
HTML_DOC=$(curl -s -X POST http://localhost:8000/documents/upload \
  -H "Authorization: Bearer $TOKEN1" \
  -F "file=@.agent/validation/fixtures/test_document.html" | jq -r '.id')

# Wait for processing
sleep 10

# Check status and content
curl -s http://localhost:8000/documents \
  -H "Authorization: Bearer $TOKEN1" | jq ".[] | select(.id==\"$HTML_DOC\")"
```
**Acceptance Criteria:**
- HTML uploaded successfully
- Text extracted (HTML tags stripped or converted to Markdown)
- Content is readable (not full of HTML tags)
- Chunks created

---

### API-50: Reject unsupported file type
**Steps:**
```bash
# Try to upload .py file
curl -s -o /dev/null -w "%{http_code}" -X POST http://localhost:8000/documents/upload \
  -H "Authorization: Bearer $TOKEN1" \
  -F "file=@backend/app/main.py"
```
**Acceptance Criteria:**
- HTTP status code is 400 (Bad Request)
- Error message clearly states unsupported file type
- Lists allowed extensions in error message

---

### API-51: Cascade delete removes chunks
**Steps:**
```bash
# Upload a document
DOC_ID=$(curl -s -X POST http://localhost:8000/documents/upload \
  -H "Authorization: Bearer $TOKEN1" \
  -F "file=@.agent/validation/fixtures/test_document.txt" | jq -r '.id')

# Wait for processing
sleep 10

# Get chunk count
CHUNKS_BEFORE=$(curl -s http://localhost:8000/documents \
  -H "Authorization: Bearer $TOKEN1" | jq ".[] | select(.id==\"$DOC_ID\") | .chunk_count")

# Delete document
curl -s -X DELETE http://localhost:8000/documents/$DOC_ID \
  -H "Authorization: Bearer $TOKEN1"

# Verify chunks are gone (requires database access or retrieval attempt)
# This test verifies via absence in search results
```
**Acceptance Criteria:**
- Document deletes successfully
- Chunks are automatically removed (CASCADE)
- No orphaned chunks remain in database
- Storage file is also removed

---

### E2E-30: Upload and process multi-format documents
**Steps:**
1. Sign in as test@test.com
2. Navigate to Documents page
3. Upload `test_document.pdf`
4. Wait for processing (status ‚Üí completed)
5. Upload `test_document.docx`
6. Wait for processing
7. Upload `test_document.html`
8. Wait for processing
9. Observe all three documents in list
10. Take screenshot

**Acceptance Criteria:**
- All three files upload successfully
- Each shows appropriate file icon (üìÑ üìù üåê)
- All transition to "completed" status
- Metadata badges appear for each
- No error messages visible
- Chunk counts are similar across formats (similar content)

---

## Edge Cases to Handle

1. **Empty PDF (no text):** Return error "No text content extracted"
2. **Scanned PDF (images only):** Fail gracefully with clear message (OCR not supported)
3. **Password-protected PDF:** Fail with clear error message
4. **Corrupted DOCX:** Catch exception, mark document as failed
5. **HTML with only images:** Extract alt text if available, or fail gracefully
6. **Very large files:** Already handled by MAX_FILE_SIZE limit
7. **Files with wrong extension:** Validate by MIME type where possible
8. **Special characters in filenames:** Already handled by existing sanitization

---

## Libraries Comparison

### PDF Parsing Options

| Library | Pros | Cons | Choice |
|---------|------|------|--------|
| **pypdf** (PyPDF2 fork) | Actively maintained, pure Python, no dependencies | Doesn't handle scanned PDFs | ‚úÖ **Recommended** |
| pdfplumber | Better table extraction | Heavier dependencies | Good alternative |
| PyMuPDF (fitz) | Very fast, handles images | GPL license (licensing concerns) | Avoid |

### DOCX Parsing Options

| Library | Pros | Cons | Choice |
|---------|------|------|--------|
| **python-docx** | Standard library, well-documented | Only handles .docx (not .doc) | ‚úÖ **Recommended** |
| docx2txt | Simpler API | Less maintained | Alternative |

### HTML Parsing Options

| Library | Pros | Cons | Choice |
|---------|------|------|--------|
| **html2text** | Converts to Markdown (preserves structure) | Opinionated formatting | ‚úÖ **Recommended** |
| BeautifulSoup + get_text() | Simple, just text extraction | Loses structure | Good for plain text |

---

## Verification Checklist

- [ ] Libraries installed (`pypdf`, `python-docx`, `beautifulsoup4`, `html2text`)
- [ ] `extraction_service.py` created with all format handlers
- [ ] File type validation updated (accepts .pdf, .docx, .html)
- [ ] MAX_FILE_SIZE increased to 50 MB
- [ ] Ingestion service uses new extraction service
- [ ] Sample test files created (.pdf, .docx, .html)
- [ ] Cascade delete constraint verified
- [ ] Frontend shows file type icons
- [ ] Validation suite updated (5 new tests: API-47 through API-51, E2E-30)
- [ ] Upload PDF ‚Üí processes successfully
- [ ] Upload DOCX ‚Üí processes successfully
- [ ] Upload HTML ‚Üí processes successfully
- [ ] Upload unsupported type ‚Üí rejected with clear error
- [ ] Delete document ‚Üí chunks removed (cascade)
- [ ] Update PROGRESS.md with Module 6 completion

---

## Files Modified

| File | Change |
|------|--------|
| `backend/requirements.txt` | Add pypdf, python-docx, beautifulsoup4, html2text |
| `backend/app/services/extraction_service.py` | New service for multi-format text extraction |
| `backend/app/routers/documents.py` | Update ALLOWED_TYPES and ALLOWED_EXTENSIONS, increase MAX_FILE_SIZE |
| `backend/app/services/ingestion_service.py` | Use extraction_service instead of inline extraction |
| `backend/migrations/004_verify_cascade_deletes.sql` | Verify foreign key CASCADE constraint |
| `frontend/src/components/documents/DocumentList.tsx` | Add file type icons |
| `.agent/validation/fixtures/test_document.pdf` | New sample PDF file |
| `.agent/validation/fixtures/test_document.docx` | New sample DOCX file |
| `.agent/validation/fixtures/test_document.html` | New sample HTML file |
| `.agent/validation/full-suite.md` | Add 5 API tests + 1 E2E test |
| `PROGRESS.md` | Mark Module 6 as completed |

---

## Complexity Assessment

‚ö†Ô∏è **Medium** - This plan has moderate complexity:

**What makes it Medium:**
- External library dependencies (pypdf, python-docx)
- Different parsing strategies for each format
- Error handling for corrupt/unsupported files
- Testing requires creating binary test files (PDF, DOCX)
- Format-specific quirks (PDFs with images, DOCX tables, HTML structure)

**Why it's still feasible:**
- Well-established libraries with good documentation
- Clear extraction APIs (all return string)
- Existing ingestion pipeline just needs text input
- No changes to chunking, embedding, or retrieval
- Validation is straightforward (upload ‚Üí check text extracted)

**Risks:**
- Scanned PDFs (images) won't work without OCR (document limitation)
- Some PDFs may have complex layouts that don't extract well (accept limitation)
- DOCX tables might not format perfectly (acceptable)
- Large files could slow processing (already have size limit)

An agent should be able to complete this in 1-2 passes with testing of various document formats.

---

## Success Criteria

Module 6 is complete when:
1. Backend accepts .pdf, .docx, .html files in addition to .txt and .md
2. Text extraction works for all supported formats
3. Extracted text flows through existing pipeline (metadata, chunking, embedding)
4. Corrupt/unsupported files fail gracefully with clear errors
5. Cascade deletes properly clean up chunks when document deleted
6. Frontend shows appropriate icons for different file types
7. All validation tests pass (API-47 through API-51, E2E-30)
8. Upload PDF/DOCX/HTML ‚Üí metadata extracted ‚Üí chunks created ‚Üí searchable
9. Sample test files available for future testing
10. Documentation updated in PROGRESS.md

**Next Module:** Module 7 - Hybrid Search & Reranking (keyword + vector + RRF + reranking)
